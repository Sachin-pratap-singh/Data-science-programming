{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the fashion-mnist dataset from the tensorflow datasets or download from kaggle(https://www.kaggle.com/zalando-research/fashionmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential,model_from_json\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "x_test1=X_test\n",
    "y_test1=y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Normalize the data - briefly comment why we need to normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full=X_train_full/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize to data to scale the values between 0 and 1, that helps the model train faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the dataset into train(50,000), valid(10,000) and test sets(10,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid = X_train_full[:50000], X_train_full[50000:]\n",
    "y_train,y_valid = y_train_full[:50000], y_train_full[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train data: (50000, 28, 28) (50000,)\n",
      "shape test data: (10000, 28, 28) (10000,)\n",
      "shape validation data (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"shape train data:\",X_train.shape,y_train.shape)\n",
    "print(\"shape test data:\",X_test.shape,y_test.shape)\n",
    "print(\"shape validation data\",X_valid.shape,y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Reshape the input data to a 2D for MLP(Multi Layer perceptron) and tensor(4d) for DNN(Deep Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 28, 28), (50000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train=X_train.reshape(50000,784)\n",
    "X_valid=X_valid.reshape(10000,784)\n",
    "X_test=X_test.reshape(10000,784)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Build a Neural Network Multi-Layer Perceptron Classifier model (you can use sklearn neural network MLP Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp_clf=mlp_clf.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=mlp_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[635,   7,  28, 104,  16,   0, 189,   0,  20,   1],\n",
       "       [  0, 952,   6,  29,  11,   0,   2,   0,   0,   0],\n",
       "       [  5,   4, 509,  12, 405,   1,  59,   0,   5,   0],\n",
       "       [ 15,  33,  14, 788, 115,   1,  26,   1,   7,   0],\n",
       "       [  0,   3,  31,  14, 935,   0,  12,   0,   5,   0],\n",
       "       [  0,   1,   0,   1,   1, 356,   0, 207,  88, 346],\n",
       "       [ 72,   6,  74,  52, 444,   0, 324,   1,  27,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 800,   1, 199],\n",
       "       [  1,   1,   2,   7,  31,   4,  23,   9, 921,   1],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   5,   2, 993]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Experiment with the architecture of the MLP classfier in the 3rd part and report if you seen any improvement in the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(100,20,10),\n",
    "                       max_iter=1000)\n",
    "\n",
    "dnn_clf.fit(X_train,y_train)\n",
    "y_pred=dnn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8521\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(100,40,30,20,10),\n",
    "                       max_iter=100)\n",
    "\n",
    "dnn_clf.fit(X_train,y_train)\n",
    "y_pred1=dnn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8258\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73656740\n",
      "Validation score: 0.837400\n",
      "Iteration 2, loss = 0.45174189\n",
      "Validation score: 0.845400\n",
      "Iteration 3, loss = 0.39374309\n",
      "Validation score: 0.858200\n",
      "Iteration 4, loss = 0.36391037\n",
      "Validation score: 0.867800\n",
      "Iteration 5, loss = 0.33524941\n",
      "Validation score: 0.879400\n",
      "Iteration 6, loss = 0.31745009\n",
      "Validation score: 0.873800\n",
      "Iteration 7, loss = 0.30446375\n",
      "Validation score: 0.883600\n",
      "Iteration 8, loss = 0.29312311\n",
      "Validation score: 0.881000\n",
      "Iteration 9, loss = 0.27922960\n",
      "Validation score: 0.882400\n",
      "Iteration 10, loss = 0.26589668\n",
      "Validation score: 0.889800\n",
      "Iteration 11, loss = 0.25640957\n",
      "Validation score: 0.881200\n",
      "Iteration 12, loss = 0.24903315\n",
      "Validation score: 0.882000\n",
      "Iteration 13, loss = 0.24516435\n",
      "Validation score: 0.890400\n",
      "Iteration 14, loss = 0.23290224\n",
      "Validation score: 0.886800\n",
      "Iteration 15, loss = 0.22881059\n",
      "Validation score: 0.884400\n",
      "Iteration 16, loss = 0.22177589\n",
      "Validation score: 0.891200\n",
      "Iteration 17, loss = 0.21677772\n",
      "Validation score: 0.889000\n",
      "Iteration 18, loss = 0.20868550\n",
      "Validation score: 0.890800\n",
      "Iteration 19, loss = 0.20438621\n",
      "Validation score: 0.886600\n",
      "Iteration 20, loss = 0.19369470\n",
      "Validation score: 0.893200\n",
      "Iteration 21, loss = 0.19096170\n",
      "Validation score: 0.888800\n",
      "Iteration 22, loss = 0.18944594\n",
      "Validation score: 0.892800\n",
      "Iteration 23, loss = 0.18314025\n",
      "Validation score: 0.890200\n",
      "Iteration 24, loss = 0.17745151\n",
      "Validation score: 0.877200\n",
      "Iteration 25, loss = 0.17231882\n",
      "Validation score: 0.893600\n",
      "Iteration 26, loss = 0.16398203\n",
      "Validation score: 0.892800\n",
      "Iteration 27, loss = 0.16452569\n",
      "Validation score: 0.894200\n",
      "Iteration 28, loss = 0.15998947\n",
      "Validation score: 0.889600\n",
      "Iteration 29, loss = 0.15556488\n",
      "Validation score: 0.896400\n",
      "Iteration 30, loss = 0.15017667\n",
      "Validation score: 0.883600\n",
      "Iteration 31, loss = 0.14596068\n",
      "Validation score: 0.893600\n",
      "Iteration 32, loss = 0.14572739\n",
      "Validation score: 0.891400\n",
      "Iteration 33, loss = 0.14260898\n",
      "Validation score: 0.888200\n",
      "Iteration 34, loss = 0.13587229\n",
      "Validation score: 0.893400\n",
      "Iteration 35, loss = 0.13586883\n",
      "Validation score: 0.897800\n",
      "Iteration 36, loss = 0.12936080\n",
      "Validation score: 0.894200\n",
      "Iteration 37, loss = 0.12438255\n",
      "Validation score: 0.892400\n",
      "Iteration 38, loss = 0.12513762\n",
      "Validation score: 0.895000\n",
      "Iteration 39, loss = 0.12066795\n",
      "Validation score: 0.891600\n",
      "Iteration 40, loss = 0.11757380\n",
      "Validation score: 0.886600\n",
      "Iteration 41, loss = 0.11522467\n",
      "Validation score: 0.889800\n",
      "Iteration 42, loss = 0.11112707\n",
      "Validation score: 0.897600\n",
      "Iteration 43, loss = 0.10814210\n",
      "Validation score: 0.889600\n",
      "Iteration 44, loss = 0.10788638\n",
      "Validation score: 0.894400\n",
      "Iteration 45, loss = 0.11097344\n",
      "Validation score: 0.892200\n",
      "Iteration 46, loss = 0.10260550\n",
      "Validation score: 0.894400\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy of the model is: 0.8486\n"
     ]
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(150,50,10),verbose=True,\n",
    "                       max_iter=1000,\n",
    "                       early_stopping=True)\n",
    "\n",
    "\n",
    "dnn_clf.fit(X_train,y_train)\n",
    "y_pred2=dnn_clf.predict(X_test)\n",
    "print(\"Accuracy of the model is:\",accuracy_score(y_test,y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8521\n"
     ]
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(150,50,10),\n",
    "                       max_iter=1000,\n",
    "                       early_stopping=True,activation = 'tanh',\n",
    "                       solver='sgd')\n",
    "\n",
    "\n",
    "dnn_clf.fit(X_train,y_train)\n",
    "y_pred3=dnn_clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. Build a basic sequential deep learning model(CNN) and compare the accuracy with MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "x_train=X_train\n",
    "x_test=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 28, 28, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    X_valid= X_valid.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    X_valid= X_valid.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "X_valid= X_valid.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "X_valid/=255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from tensorflow.keras import utils\n",
    "#y_train = utils.to_categorical(y_train, num_classes)\n",
    "#y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "print(y_train[0])\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "y_valid=keras.utils.to_categorical(y_valid,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28, 1), (10000, 10))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(learning_rate=0.25),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 28, 28, 1), (50000, 10))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 2.3023 - accuracy: 0.1069 - val_loss: 2.3016 - val_accuracy: 0.1626\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 2.2842 - accuracy: 0.1611 - val_loss: 2.1411 - val_accuracy: 0.4279\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 1.3137 - accuracy: 0.5203 - val_loss: 0.7446 - val_accuracy: 0.7282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fdd5210e80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 26.479896545410156\n",
      "Test accuracy: 0.6258000135421753\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Experiment with the architecture of CNN and report if you see any improvement in the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(learning_rate=0.25),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 2.3017 - accuracy: 0.1129 - val_loss: 2.2989 - val_accuracy: 0.1317\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.9410 - accuracy: 0.3234 - val_loss: 0.9184 - val_accuracy: 0.6823\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.8157 - accuracy: 0.6982 - val_loss: 0.6406 - val_accuracy: 0.7594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fdd54c39e8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 60.857261657714844\n",
      "Test accuracy: 0.6712999939918518\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - 83s 106ms/step - loss: 0.8948 - accuracy: 0.6692 - val_loss: 0.5832 - val_accuracy: 0.7835\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 88s 113ms/step - loss: 0.5868 - accuracy: 0.7778 - val_loss: 0.5328 - val_accuracy: 0.8006\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 83s 106ms/step - loss: 0.5312 - accuracy: 0.8019 - val_loss: 0.4751 - val_accuracy: 0.8239\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 86s 111ms/step - loss: 0.4987 - accuracy: 0.8139 - val_loss: 0.4488 - val_accuracy: 0.8349\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 0.4717 - accuracy: 0.8243 - val_loss: 0.4362 - val_accuracy: 0.8374\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 90s 116ms/step - loss: 0.4489 - accuracy: 0.8355 - val_loss: 0.4069 - val_accuracy: 0.8534\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 94s 120ms/step - loss: 0.4282 - accuracy: 0.8417 - val_loss: 0.3908 - val_accuracy: 0.8561\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 93s 119ms/step - loss: 0.4065 - accuracy: 0.8509 - val_loss: 0.3755 - val_accuracy: 0.8617\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 90s 116ms/step - loss: 0.3871 - accuracy: 0.8580 - val_loss: 0.3510 - val_accuracy: 0.8725\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 0.3685 - accuracy: 0.8638 - val_loss: 0.3410 - val_accuracy: 0.8760\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 93s 119ms/step - loss: 0.3550 - accuracy: 0.8692 - val_loss: 0.3286 - val_accuracy: 0.8803\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.3389 - accuracy: 0.8766 - val_loss: 0.3179 - val_accuracy: 0.8822\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 91s 117ms/step - loss: 0.3246 - accuracy: 0.8798 - val_loss: 0.3018 - val_accuracy: 0.8884\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 80s 103ms/step - loss: 0.3145 - accuracy: 0.8846 - val_loss: 0.2945 - val_accuracy: 0.8921\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 80s 103ms/step - loss: 0.3002 - accuracy: 0.8881 - val_loss: 0.2883 - val_accuracy: 0.8947\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.2933 - accuracy: 0.8918 - val_loss: 0.2824 - val_accuracy: 0.8958\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.2835 - accuracy: 0.8951 - val_loss: 0.2789 - val_accuracy: 0.8991\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.2740 - accuracy: 0.8975 - val_loss: 0.2802 - val_accuracy: 0.8981\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.2672 - accuracy: 0.9014 - val_loss: 0.2815 - val_accuracy: 0.8964\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.2627 - accuracy: 0.9031 - val_loss: 0.2634 - val_accuracy: 0.9042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fdd636def0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=20\n",
    "batch_size = 64\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 34.37784957885742\n",
      "Test accuracy: 0.8190000057220459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Evaluate MLPClassifier and CNN using F1 score values and accuracy(only for the best model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-Score Best MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1259: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8486"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(y_test1,y_pred2,pos_label='positive',average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "F1-score best CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn = model.predict(x_test)\n",
    "y_pred_cnn=np.argmax(y_pred_cnn, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7154999999999999"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(y_test1,y_pred_cnn,pos_label='positive',average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On test data set:\n",
    "Accuraccy MLP:0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data set\n",
    "Accuracy CNN:0.81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10. Explain the paramters to tune to reduce the risk of overfitting in deep learning models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep neural networks risk of overfitting can be reduced by adding Dropout layer to model which multiplies 0 to random nodes weights and make them non-influential. Also we may use rgularisation in some cases to reduce the overfitting(l1 or l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Optional Explain the meaning of Precision, Recall and F1-Score and why these are used to evaluate Classification models (instead of using Accuracy as a metric). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision : Precision is the ratio that explains the positive predictions with respect to all predicted positives.   \n",
    "Recall: Recall is the ratio that explains the positive prediction with respect to all possible positives(actual positives).  \n",
    "  F1-Score: F1 score is the harmonic mean or weighted vaerage of precision and recall.  \n",
    "Theses metrics are used to evaluate the models because the cost of precision and recall might vary on the case by case basis.\n",
    "In some cases we might need high precision value While as in some cases high recall. Accuracy might me important when cost of precision and recall are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
